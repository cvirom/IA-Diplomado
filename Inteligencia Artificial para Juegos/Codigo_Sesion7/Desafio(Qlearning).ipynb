{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura de Especialización en Desarrollo de Aplicaciones con Inteligencia Artificial  -  Inteligencia Artificial para Juegos (Game AI)\n",
    "\n",
    "Dr. Edwin Villanueva (evillatal@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje por Refuerzo  Q-learning   (DESAFIO AL FINAL)\n",
    "\n",
    "El presente desafio aborda la experimentacion de agentes de aprendizaje por refuerzo Q-learning en entornos grid. La implementacion de la clase del entorno GridEnvironment y el agente Q-learning ya estan implementadas. Al final del notebook deberas responder a las preguntas planteadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clase <b>GridEnvironment</b>\n",
    "\n",
    "La clase GridEnvironment define un entorno MDP (Proceso de Desiciones de Markov) para entornos grids (laberintos), como el ejemplo usado en clase. Las probabilidades de transicion son 0.8 para moverse en la dirección pretendida y 0.1 de moverse a un estado lateral. El constructor recibe:\n",
    "\n",
    "- grid: un array de listas de numeros definiendo los rewards del grid del entorno. Valores None indican un obstaculo\n",
    "- terminals: lista de estados terminales\n",
    "- initial: estado inicial\n",
    "- gamma: factor de descuento\n",
    "\n",
    "La clase mantiene el estado actual (current_state), el cual se inicializa en estado \"initial\" y se modifica con cada paso que se dé en el entorno (llamada a step()), devolviendo el nuevo estado, el reward y un flag 'done' que indica si el entorno ha caido en un estado terminal. El modelo de transicion de cada estado es accesible a travez de la funcion T(s,a) que devuelve una lista de tuplas (prob, s') para cada estado vecino s' del estado s ejecutando la accion a (prob es la probabilidad de transicionar de s a s' con accion a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "EAST, NORTH, WEST, SOUTH = (1, 0), (0, 1), (-1, 0), (0, -1)\n",
    "LEFT, RIGHT = +1, -1\n",
    "        \n",
    "class GridEnvironment:\n",
    "    def __init__(self, grid, terminals, initial=(0, 0), gamma=.9):\n",
    "        grid.reverse()     # para que fila 0 sea la de abajo, no la de arriba\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        self.grid = grid\n",
    "        self.initial_state = initial\n",
    "        self.current_state = initial\n",
    "        self.terminals = terminals\n",
    "        self.gamma = gamma\n",
    "        self.actionlist = [EAST, NORTH, WEST, SOUTH] \n",
    "\n",
    "        self.rewards = {}        # diccionario de rewards\n",
    "        self.states = set()     # conjunto de estados diferentes\n",
    "        for x in range(self.cols):   # obtiene todos los estados y rewards del grid\n",
    "            for y in range(self.rows):\n",
    "                if grid[y][x]:  # Si la celda no es None (Prohibida), agrega el estado y reward\n",
    "                    self.states.add((x, y))\n",
    "                    self.rewards[(x, y)] = grid[y][x]\n",
    "            \n",
    "        self.transition_probs = {}  # almacena los diccionarios de probabilidades de transicion\n",
    "        for s in self.states:\n",
    "            self.transition_probs[s] = {}  # diccionario de probabilidades de transicion de los vecinos de estado s\n",
    "            for a in self.actionlist:\n",
    "                self.transition_probs[s][a] = self.get_transition_probs(s, a)\n",
    "                \n",
    "    def get_transition_probs(self, state, action): \n",
    "        # Hay 0.8 de probabilidad de moverse en la dirección pretendida y 0.1 de moverse por cada lateral. \n",
    "        if action:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state, self.turn_right(action))),\n",
    "                    (0.1, self.go(state, self.turn_left(action)))]\n",
    "        else:\n",
    "            return [(0.0, state)]\n",
    "        \n",
    "    def go(self, state, direction):\n",
    "        \"\"\"Retorna el estado que resultaria de ir en la direccion pasada, si el ambiente fuese deterministico \"\"\"\n",
    "        state1 = tuple(map(operator.add, state, direction))\n",
    "        return state1 if state1 in self.states else state    \n",
    "    \n",
    "    def turn_heading(self, heading, inc, headings=[EAST, NORTH, WEST, SOUTH]):\n",
    "        return headings[(headings.index(heading) + inc) % len(headings)]\n",
    "\n",
    "    def turn_right(self, heading):\n",
    "        return self.turn_heading(heading, RIGHT)\n",
    "\n",
    "    def turn_left(self, heading):\n",
    "        return self.turn_heading(heading, LEFT) \n",
    "    \n",
    "    def T(self, s, a):  # Retorna los estados vecinos y sus prob de transicion, tuplas (prob, s'), para el estado  s y accion a\n",
    "        return self.transition_probs[s][a] if a else [(0.0, s)]\n",
    "\n",
    "    def R(self, state): # retorna el reward de un estado\n",
    "        return self.rewards[state]    \n",
    "    \n",
    "    def actions(self, state): # retorna la lista de acciones posibles en un estado \n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actionlist    \n",
    "    \n",
    "    def reset(self):  # Reseta el Entorno\n",
    "        self.current_state = self.initial_state\n",
    "        return self.current_state, self.rewards[self.current_state]\n",
    "    \n",
    "    def step(self, action): # Ejecuta un paso el entorno. Retorna el nuevo estado, el reward y flag de que es estado terminal\n",
    "        x = random.uniform(0, 1)\n",
    "        cumulative_probability = 0.0\n",
    "        for probability_state in self.T(self.current_state, action):\n",
    "            probability, next_state = probability_state\n",
    "            cumulative_probability += probability\n",
    "            if x < cumulative_probability:\n",
    "                break\n",
    "        self.current_state = next_state\n",
    "        done = True if current_state in self.terminals else False\n",
    "        return self.current_state, self.rewards[self.current_state], done\n",
    "    \n",
    "    def to_grid(self, mapping):\n",
    "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                               for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n",
    "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n",
    "    \n",
    "    def print_policy(self, policy):\n",
    "        \"\"\"Imprime la politica\"\"\"\n",
    "        header=None\n",
    "        sep='   '\n",
    "        numfmt='{}'\n",
    "        table = self.to_arrows(policy)\n",
    "        justs = ['rjust' if hasattr(x, '__int__') else 'ljust' for x in table[0]]\n",
    "\n",
    "        if header:\n",
    "            table.insert(0, header)\n",
    "\n",
    "        table = [[numfmt.format(x) if hasattr(x, '__int__') else x for x in row]\n",
    "                 for row in table]\n",
    "\n",
    "        sizes = list(\n",
    "            map(lambda seq: max(map(len, seq)),\n",
    "                list(zip(*[map(str, row) for row in table]))))\n",
    "\n",
    "        for row in table:\n",
    "            print(sep.join(getattr(\n",
    "                str(x), j)(size) for (j, size, x) in zip(justs, sizes, row)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Entorno para experimentar </b>\n",
    "Para experimentar,  se usará el entorno MDP definido abajo. El factor de descuento es $\\gamma = 0.9$ (en los ejemplos de clase se usó $\\gamma = 1$). Las recompensas son **-0.1** en estados no terminales y **+5** y **-5** en estados terminales.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el grid que se vio en clase\n",
    "#grid = [[-0.04, -0.04, -0.04, +1],\n",
    "#        [-0.04,  None, -0.04, -1],\n",
    "#        [-0.04, -0.04, -0.04, -0.04]]\n",
    "\n",
    "# el grid de este desafio\n",
    "grid = [\n",
    "    [None, None, None, None, None, None, None, None, None, None, None], \n",
    "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None, +5.0, None], \n",
    "    [None, -0.1, None, None, None, None, None, None, None, -0.1, None], \n",
    "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None], \n",
    "    [None, -0.1, None, None, None, None, None, None, None, None, None], \n",
    "    [None, -0.1, None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None], \n",
    "    [None, -0.1, None, None, None, None, None, -0.1, None, -0.1, None], \n",
    "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None, -0.1, None], \n",
    "    [None, None, None, None, None, -0.1, None, -0.1, None, -0.1, None], \n",
    "    [None, -5.0, -0.1, -0.1, -0.1, -0.1, None, -0.1, None, -0.1, None], \n",
    "    [None, None, None, None, None, None, None, None, None, None, None]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase <b>QLearningAgent</b>\n",
    "\n",
    "Esta clase define un agente exploratorio Q-learning. Este evita aprender el modelo de transicion ya que los Q-valores de un estado-action puede ser relacionado directamente a los Q-valores de los estado-action vecinos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \n",
    "    def __init__(self, mdp, Ne, Rplus, alpha=None):\n",
    "\n",
    "        self.gamma = mdp.gamma    # factor de descuento (definido en el MDP)\n",
    "        self.terminals = mdp.terminals   # estados terminales (definido en el MDP)\n",
    "        self.all_act = mdp.actionlist  # acciones posibles\n",
    "        self.Ne = Ne        # limite de iteraciones de la funcion de exploracion\n",
    "        self.Rplus = Rplus  # Recompensa que tienen los estados (o q-estados) antes del limite de iteraciones Ne\n",
    "        self.Q = defaultdict(float)   # almacena los q-valores\n",
    "        self.Nsa = defaultdict(float) # almacena la tabla de frecuencias state-action\n",
    "        self.s = None    # estado anterior\n",
    "        self.a = None    # ultima accion ejecutada\n",
    "        self.r = None    # recompensa de estado anterior\n",
    "\n",
    "        if alpha:\n",
    "            self.alpha = alpha   # alpha es la taza de aprendizaje. Debe disminuir con el numero de visitas al estado para que las utilidades converjan\n",
    "        else:\n",
    "            self.alpha = lambda n: 1./(1+n)  # udacity video\n",
    "\n",
    "    def f(self, u, n): \n",
    "        \"\"\" Funcion de exploracion. Retorna un valor de utilidad fijo (Rplus) hasta que el agente visita Ne veces el state-action \"\"\"\n",
    "        if n < self.Ne:\n",
    "            return self.Rplus\n",
    "        else:\n",
    "            return u\n",
    "\n",
    "    def actions_in_state(self, state):\n",
    "        \"\"\" Retorna el conbjunto de acciones posibles del estado pasado. Util para max y argmax. \"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.all_act\n",
    "\n",
    "    # Programa del agente Q-learning    \n",
    "    def __call__(self, percept):    \n",
    "        \"\"\" Este es el programa del agente que es llamado en cada step, recibe un percept y retorna una accion \"\"\"\n",
    "        s1, r1 = self.update_state(percept)\n",
    "        Q, Nsa, s, a, r = self.Q, self.Nsa, self.s, self.a, self.r\n",
    "        alpha, gamma, terminals = self.alpha, self.gamma, self.terminals,\n",
    "        actions_in_state = self.actions_in_state\n",
    "\n",
    "        if s in terminals:\n",
    "            Q[s, None] = r1\n",
    "        if s is not None:\n",
    "            Nsa[s, a] += 1\n",
    "            Q[s, a] += alpha(Nsa[s, a]) * (r + gamma * max(Q[s1, a1] for a1 in actions_in_state(s1)) - Q[s, a])\n",
    "        if s in terminals:\n",
    "            self.s = self.a = self.r = None\n",
    "        else:\n",
    "            self.s, self.r = s1, r1\n",
    "            self.a = max(actions_in_state(s1), key=lambda a1: self.f(Q[s1, a1], Nsa[s1, a1])) # funciona como argmax, devuelve la accion con mayor f\n",
    "        return self.a\n",
    "\n",
    "    def update_state(self, percept):\n",
    "        ''' To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward)'''\n",
    "        return percept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando el agente  <b>Q-learning</b>\n",
    "\n",
    "Vamos a instanciar un agente Q-learning para aprender una politica en nuestro entorno de prueba \"grid\". Los parametros del agente son los siguientes: **Ne = 10**, **Rplus = 2**, **alpha** como dado en la nota de pie del libro **pagina 837**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia el entorno del grid\n",
    "#environment = GridEnvironment(grid, terminals=[(3, 2), (3, 1)], initial=(0, 0), gamma=0.9) # grid de la clase\n",
    "environment = GridEnvironment(grid, terminals=[(1, 1), (9, 9)], initial=(3, 1), gamma=0.9) # \n",
    "\n",
    "# Instancia un agente Q-learning \n",
    "agent = QLearningAgent(environment, Ne=10, Rplus=2, alpha=lambda n: 60./(59+n)) \n",
    "\n",
    "# Ejecuta 10000 episodios del agente en el entorno\n",
    "TRIALS = 10000      \n",
    "for e in range(TRIALS):   # Por caa trial\n",
    "    current_state, current_reward = environment.reset()\n",
    "    score_trial = current_reward   # el escore del episodio es la suma acumulada de rewards en el episodio \n",
    "    while True:  # ejecuta steps del entorno hasta llegar a un estado terminal\n",
    "        percept = (current_state, current_reward)  # la percepcion del agente es la tupla (state, reward)\n",
    "        action  = agent(percept)  # llama al programa del agente, pasandole el percept y espera una accion a ejecutar\n",
    "        current_state, current_reward, done = environment.step(action) # ejecuta la accion en el entorno, \n",
    "        score_trial += current_reward\n",
    "        if done:\n",
    "            print(\"Trial: {}/{}, score: {}\".format(e, TRIALS, score_trial))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos el diccionario de los Q-valores aprendidos. Las claves son pares state-action. Las diferentes acciones corresponden a:\n",
    "\n",
    "NORTH = (0, 1)  \n",
    "SOUTH = (0,-1)  \n",
    "WEST = (-1, 0)  \n",
    "EAST = (1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qvalues = agent.Q\n",
    "#print(Qvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DESAFIO:\n",
    "\n",
    "\n",
    "<b>1) Cree una funcion para extraer las utilidades (U) de los estados a partir de los Q-valores obtenidos por el agente. LLame esta funcion: get_utilities_from_qvalues(Qvalues). Pruebela en el resultado anterior (agent.Q)</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utilities_from_qvalues(mdp, Q):\n",
    "    \"\"\"Dado un MDP y una funcion de utilidad Q, determina los valores de utilidad de los estados. \"\"\"\n",
    "    U = {}\n",
    "    for s in mdp.states:\n",
    "        if s not in mdp.terminals:\n",
    "            U[s] =  -np.inf\n",
    "            for a in mdp.actionlist:\n",
    "                if Q[(s, a)] > U[s] : \n",
    "                    U[s] = Q[(s, a)]\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = get_utilities_from_qvalues(environment, agent.Q)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>2)  Cree una funcion para extraer la politica a partir de los Q-valores obtenidos por el agente. LLame esta funcion: get_policy_from_qvalues(Qvalues). Pruebela en el resultado anterior (agent.Q)</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_from_qvalues(mdp, Q):\n",
    "    \"\"\"Dado un MDP y una funccion de utilidad Q, determina la mejor politica. \"\"\"\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        if s not in mdp.terminals:\n",
    "            pi[s] = max(mdp.actionlist, key=lambda a: Q[(s,a)])\n",
    "        else:\n",
    "            pi[s] = None\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_qlearning = get_policy_from_qvalues(environment, Qvalues)\n",
    "environment.print_policy(pi_qlearning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Cree una función para comparar dos politicas y devolver el numero de estados en que coinciden las politicas. Llame dicha funcion: comparare_policies(policy1, policy2). Pruebe la función con policy1 = pi (la politica obtenida en la pregunta 1) y policy2 = la política optima de resolver el MDP con el metodo value_iteration <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computa e imprime la politica optima usando value_iteration (este tiene acceso al modelo de transicion) \n",
    "from mdp import *  # mdp tiene la implementacion de value iteration\n",
    "pi_valueiteration = best_policy(environment, value_iteration(environment, .001))\n",
    "environment.print_policy(pi_valueiteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparare_policies(policy1, policy2):\n",
    "    # TODO:  Esta es la parte que debe completar. Debe devolver el numero de acciones coincidentes entre politicas\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_acciones_coincidentes = comparare_policies(pi_qlearning, pi_valueiteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Ejecute el agente (10000 trials) con los valores siguientes de Ne: {0, 1, 5, 10} y compare las politicas obtenidas en relacion a la politica optima de value_iteration (pi_valueiteration). Cuál valor de Ne genera resultados mas proximos a la politica optima? Explique por qué <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
